---
  title: "IBIO*6000 - Ecology & Behaviour"
  author: "Bryan Vandenbrink"
  date: "08/10/2021"
  output: html_document
---

This is an R markdown document intended to perform an iterative example of processing a text file containing a set of comma spaced value data downloaded from BOLD.

If you'd like to know more about R Markdown vs R scripts, here's a link to a tutorial demonstrating the differences between R scripts and R Markdown.

https://rmarkdown.rstudio.com/articles_intro.html

This is an example of a chunk of R code embedded within the R markdown file. Note that code chunks are inserted with the tilde character repeated three times. The same tilde character x3 is also used to close out the R code chunk.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In the example above, the Knitr package is called and will run each chunk of R code in the document and append the results next to the R code chunk.

```{r tidyverse message=FALSE}
# This is an example of a comment within a chunk of R code.
# The code below checks if the tidyverse package is installed
# and if it is, installs it.
if (!require("tidyverse")) install.packages("tidyverse")
# Otherwise, it loads the tidyverse package.
library(tidyverse)
```

```{r viridis message=FALSE}
# Checks for viridis library and if not present, installs it
# viridis is a library for generating graphs for the colour blind
if (!require("viridis")) install.packages("viridis")
# Load the required library
library(viridis)
```

```{r skimr message=FALSE}
# Checks for skimr and if not present, installs it
if (!require("skimr")) install.packages("skimr")
# Load the required library
library(skimr)
```

Here is a hyperlink to the data import tutorial:
https://r4ds.had.co.nz/data-import.html

Now that we've loaded the tidyverse package, which includes methods and functions for processing csv files we will begin loading the csv file.

As indicated in the tutorial, you have read_csv (Data separated by comma's), read_csv2 (Data separated by semi-colon's), read_tsv (Data separated by tabs). In the case of BOLD systems data, tab separated is one of the format's that specimen data is available as so we will be using that.

Note on file path and directories. The "/" is an escape character within R so it will be processed that way. When entering the directory for where your data is located, use a backslash character instead.

If you're wondering what the difference is, here's a link to an explanation of the difference between the two:
https://sites.cs.ucsb.edu/~pconrad/topics/BackslashVsForwardSlash/

```
# Here we will load the available data into specimen_data.
specimen_data <- read_tsv("E:/2021_UoG/IBIO 6000/src/Data/DS-ITLP.txt")
# All the data from the tab separated format file should now be loaded into R.
```

Note that within the R script the tidyverse library automatically assigns it's own interpreted data types to the columns. For an explanation of date types in R, here's a handy reference: https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/

In my case, when importing the BOLD data from a tab separated file the data structures/types used were character, character or chr, double class or a double-precision floating point number, and finally logical or lgl.

Note that values that are blank are considered to be N/A or false, hence the existence of the logical or lgl data type within the data.

This is an important consideration from the point of data normalization. Namely that some values within columns might not be filled in and as a result will show up as N/A.

```{r }
# This will produce a basic graph using ggplot.
# The graph it will generate is a composition of recordID and the captured insect family name within the order lepidoptera. (Since all captured specimens were moths.)
specimen_data %>% skimr::skim()

ggplot(data = specimen_data) +
  geom_point(mapping = aes(x = recordID, y = family_name))
# This plot puts the recordID along the x axis and the family name in the y axis.
# Also so far, we haven't sorted out any N/A's yet.
```


```{r }
# Or if we play with colour within aes() or a defined aesthethic, in this case color for sub-family name.
ggplot(data = specimen_data) +
  geom_point(mapping = aes(x = subfamily_name, y = recordID, color = family_name))
# Generally speaking though, this isn't useful as it only produces a plot containing a unique recordID, and family and sub-family along the x axis.
# What we want instead, is a representation of the approximate distribution of the available 241 records.
# Namely, we want to know how many unique families of the lepidoptera there are.
# To do that, we need to do a count. And since regular expressions are a wondeful sorting tool, we'll look at doing that in the next code fragment.
```

  
```{r }
# In order to proceed, let's get a count of the the number of total rows of available data there are using nrow()
nrow(specimen_data)
# Great, now that we have a count of the number of rows let's store it as a variable.
num_specimens <- nrow(specimen_data)
# We're going to keep this variable stored as we'll use it for the min_limit and max_limit of our graphs later.
# But next up, we need to do some regexing. (Searching for, matrching patterns, etc)
# We'll also use regular expressions to perform data normalization to clean up the available data.
# Namely, two of the columns contain useful information:
#  1 - The location of the US port the specimen was collected
#  2 - The suspected point of origin for where the specimen originally hitched a ride

# However, let's see if we can remove N/A's using regular expressions
# Specifically, we want to take all data from specimen_data matching a rule where family cannot be N/A
# Note that I used Tortricidae as an example from data within the rows that I know to be there.
filter(specimen_data, family == "Tortricidae", na.rm = TRUE)
# Hmm, that seemed to fail. filter is only intended to be used for atomic and list type values.

# Let's try using na.omit() and assign
#wholespecimen_data <- na.omit(specimen_data)
# Yikes, that chopped out a lot of the data. What gives?
# Well, it has to do with na.omit() removing _anything_ with an N/A record in any columnn.
# We want to be a little bit more specific. So let's try pattern matching for records from a specific column.
# Specifically, any column with N/A in the family_name column. To do this, use the argument cols = c("x", "z"))
#
#wholespecimen_data <- na.omit(specimen_data, cols = "family_name")
# Hmm, that failed too. Maybe I'm using cols wrong.
#wholespecimen_data %>% na.omit(specimen_data, cols = "family_name")
# Okay, that didn't work either.
# Let's try using a compound assignment operator. 
wholespecimen_data %>% filter(!is.na(family_name))
# That didn't seem to do a thing. Let's try assigning it instead.
wholespecimen_data <- filter(specimen_data, !is.na(family_name))
# Holy cow, that did it!
# Next, let's get a count of how many that is compared to the original dataset.
nrow(wholespecimen_data)
nrow(specimen_data)
nrow(specimen_data) - nrow(wholespecimen_data)
# Interesting, so within the family column there are 11 missing identifications.
ggplot(wholespecimen_data, aes(x = family_name, y = subfamily_name)) +
  geom_point()
# Well that chart is rather drab. There are still NA's!
# Ohhh, the y axis was assigned ot subfamily_name and any NA's present show along the y axis. Bummer dude.
# So let's remove NA's using a compound assignment %>% operator)
wholewholespecimen_data <- filter(wholespecimen_data, !is.na(subfamily_name))
# Okay this is a bit convoluted but wholewholespecimen_data should have no NA's present for both family_name and subfamily_name
# Let's try that graph again!
ggplot(wholewholespecimen_data, aes(x = family_name, y = subfamily_name)) +
  geom_point()
# Very cool!
# But let's try increasing the size of points depending on the number of specimens within subfamilies
ggplot(wholewholespecimen_data, aes(x = family_name, y = subfamily_name)) +
  geom_point()
# Right away though, you might have observed something interesting.
# Namely that family_name and subfamily_name are not very useful on charts as the graph will only give you a range of non-useful data.
# Since graphs are best represented with quantifiable approximation of numbers aka atomic values aka integers
# We need some numbers! To do that, let's try performing a count of each family.
# To do that, let's start at the highest possible point.
# In the x axis, we want a range from low to high.
nrow(wholewholespecimen_data)
# And since we know from using nrow() on wholewholespecimen_data that there are 85 rows,
# we could make a graph with a min limit of 1 and a max limit of 85.
# To do that, we can use xlim() and ylim().
# So let's look at doing that. But first we need a count of each type of family present in the dataset.
table(specimen_data$family_name)
# Interesting. So this gives the number of occurrences of specimens from each family.
# As you can see from that, we have 164 specimens of "Tortricinae" alone.
table(specimen_data$subfamily_name)
# More interesting still, this is an even smaller sub-set.
# So let's try it on the wholewholespecimen_data
table(wholewholespecimen_data$family_name)
# Interesting, so from specimen_data to wholewholespecimen_data there are gaps in identification.
# Let's try it on subfamily_name too.
table(wholewholespecimen_data$subfamily_name)
# This is less interesting as it's identical to the previously generated table. But that's understandable since during
# identification there was greater resolution. In the prior example, NA's were removed. Since sorting by subfamily_name
# means further identification has already been performed the two should be identical.
# But let's see what this looks like on a chart.

```
So, it's pretty clear we need to adjust how we're working with the data. Specifically, we need to create a graph that uses the total number of rows along the y axis and where
the collected specimens fall along the x axis.

```{r }
# Since there's a lot of code executed in the previous code block, let's create a new block.
# Looking at specimen_data, it's pretty clear we have an integer representing each record/row individually. Compare that with the example dataset on mpg where the first column is the name of the vehicle.
# Now, the problem is that previously we've been trying to create a scatter plot
# with data that is not compatible. We need to either transform that data, or perform simple counts instead.
ggplot(specimen_data) +
  geom_bar(aes(family_name))
# To do that, let's try creating a bar chart instead.
# Notice how this represents the data we're after. An approximate count of the number of times a specific family is represented.
# So let's try it again with subfamily_name too.
ggplot(specimen_data) +
  geom_bar(aes(subfamily_name))
# Notice how many are NA's. Let's try with whole data instead.
# Remember that we lost some data by removing NA's.
nrow(specimen_data)
# We start with 241.
nrow(wholewholespecimen_data)
# And after removing NA's we have 85.
ggplot(wholewholespecimen_data) +
  geom_bar(aes(subfamily_name))
# Let's play with both family nad subfamily a bit.
# Run these both to see what the differences are.
ggplot(data = specimen_data) +
  geom_bar(mapping = aes(family_name, color = subfamily_name))
# And here's the data with NAs removed.
ggplot(data = wholewholespecimen_data) +
  geom_bar(mapping = aes(family_name, color = subfamily_name)) +
  theme(text = element_text(size=8), axis.text.x = element_text(angle=90, hjust=1))
```
We have many more tests/experiments to do with creating graphs/charts/etc but this should suffice for now.
Based on the available data, the most represented potentially invasive lepidoptera species found at US ports is Tortricidae and Tineidae.
What makes those two families special? Are they all from the same port of origin?
Or is more shipping from locations with those species present?
Important questions to think about, but let's ignore that for now and focus on the next steps.
The next code block section will be about using regular expressions to clean up text.
Specifically, we need to clean up the collection_note column and the notes column.
As an example, the collection_note column currently looks like this:

Border Interception, Suspected country of origin: Peru, Interception location: California

And the notes column looks like this:

Intercepted in California, United States: original id as Gelechiidae

So, look at this information objectively it's useful in two ways. The first is it tells us where the specimen was physically collected. It'd be pretty hard for any ignorant fool to argue that a specimen isn't present in an area when it was collected there, but I'm sure like the moron who launched himself on a steam rocket to prove that the earth was flat that they would try. Occam's razor about the simplest solution being true should have a similar thing about ignorance. The most ignorant among us will remain unconvinced even when facing down an extraordinary mountain of evidence that they're holding back the human species. Luckily, COVID-19 will likely go through several mutations and it'll do what viruses in nature are responsible for. Maintaining the health of overall health of a species by predominantly eliminating the slowest, the oldest, and the weakest. Morbid, but when you're not getting one of several mRNA vaccine's because a talking fat man on the radio or the news tells you not to, it's pretty clear that you're part of natural selection naturally selecting against yourself. End of rant.

On to more pressing matters, like how do we extract this information to make it useful to us?

Well, first we need to create a regular expression that takes the origin US state and country. Then we need to create a regular expression to extract the suspected point of origin country.

We're going to attempt to use tidyr to perform the separate of collection_notes into two additional columns.

```{r }
# First, let's load tidyr
library(tidyr)
# Then, we're going to use separate() from tidyr to clean-up our data on a test set.
# Let's copy wholewholespecimen_data into a test dataset
testwwspecimen_data <- wholewholespecimen_data
# Now that wholewholespecimen_data has been moved over to a test dataset, let's try separate()
# Here is the test example for separate(), sep="" indicates what character the data will be separated by.
# E.g. separate(table3,rate,sep="/",into=c("cases","pop"))
# Use is first field: dataset
# second field: column
# third: separator
# fourth: into = c for column followed by two new columns
separate(testwwspecimen_data,notes, sep=",", into = c("intercept_type", "suspected_origin"))
# You should receive a warning: Expected 2 pieces. Missing pieces filled with `NA` in 4 rows (18, 21, 70, 72)
# And if you look at the data, it's garbled. We're accidentally matching several parts of the text unintentionally.
# Let's look at data within collection_note:
# Border Interception, Suspected country of origin: Peru, Interception Location, California, USA
# As you can see, there's the first comma after "Border Interception", then another after the country, the interception location and following the state.
# So, how do we fix this?
# Let's start by looking at what we want to keep.
# That's "Peru" and "California, USA"
# So how do we selected just those two pieces of data? Also, we need to account for NA's since there are several. Also, some just indicate "Border Interception" and that's it.
# What if we used a whole block of text as the separator?
separate(testwwspecimen_data,notes, sep="Border Interception, Suspected country of origin:", into = c("intercept_type","suspected_origin"))
# You should recieve a warning: Expected 2 pieces. Missing pieces filled with `NA` in 84 rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].
# And looking at the data...
# intercept_type is filled with "Intercepted in California,United States: original id as Gelechiidae
# And suspected_origin is filled with NA's.
# Not ideal obvs.
# So, what we might do is fall back on the original idea of using regular expressions.
```

First, let's take note of every possible value for the "notes" column by looking at the data.
There's the fairly common (fits most cell values):
 Border Interception
 Border Interception, Suspect country of origin: Puerto Rico
 Border Interception, Suspected country of origin: Peru, Interception location: California, USA
 Border Interception, Suspection country of origin: United Kingdom of Great Britain and N. Ireland, Interception location: Maryland, USA
 Border Interception, Suspected country of origin: Hondura, Interception location: Port Everglades FL, Interception Number: APMFL130386763001
There are also NA's. But 90% of the data is in the format of the 3rd example.
Let's count the special characters present. ,::,

Let's strategize:
 We could create several regular expressions to match each case and run them one at a time.
 
 Or we could start by cleaning up the data a bit. What if we put everything after "Border Interception," in another column?
 
 Or instead of using separate(), what if we used extract()?
